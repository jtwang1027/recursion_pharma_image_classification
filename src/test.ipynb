{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from train import train, load_model\n",
    "from models import CustomDensenet, CustomVit\n",
    "from dataset import Rxrx1\n",
    "from config import Config\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "from torchvision.models import vit_b_16, vit_b_32, vit_l_16, VisionTransformer, ViT_B_16_Weights\n",
    "from losses import Schedulers\n",
    "\n",
    "from torchvision import transforms\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = transforms.Compose([ transforms.RandomHorizontalFlip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = vit_b_32()\n",
    "model = vit_l_16(weights= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = 'vit_b_16'\n",
    "_model =  getattr(torchvision.models, backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "torchvision.models.vision_transformer._vision_transformer() got multiple values for keyword argument 'patch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torchvision/models/_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torchvision/models/_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[1;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torchvision/models/vision_transformer.py:641\u001b[0m, in \u001b[0;36mvit_b_16\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03mConstructs a vit_b_16 architecture from\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m`An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    :members:\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    639\u001b[0m weights \u001b[38;5;241m=\u001b[39m ViT_B_16_Weights\u001b[38;5;241m.\u001b[39mverify(weights)\n\u001b[0;32m--> 641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _vision_transformer(\n\u001b[1;32m    642\u001b[0m     patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m    643\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[1;32m    644\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[1;32m    645\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m,\n\u001b[1;32m    646\u001b[0m     mlp_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3072\u001b[39m,\n\u001b[1;32m    647\u001b[0m     weights\u001b[38;5;241m=\u001b[39mweights,\n\u001b[1;32m    648\u001b[0m     progress\u001b[38;5;241m=\u001b[39mprogress,\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    650\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: torchvision.models.vision_transformer._vision_transformer() got multiple values for keyword argument 'patch_size'"
     ]
    }
   ],
   "source": [
    "_model(pretrained = True, patch_size = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomVit(image_size =225, num_classes = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config.load_config('/Users/jasonwang/github/recursion/src/example_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, LinearLR, ExponentialLR, PolynomialLR\n",
    "from enum import Enum\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr= 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr= 1e-3)\n",
    "# sched = CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
    "# sched = LinearLR(optimizer, total_iters = 50)\n",
    "# sched = ExponentialLR(optimizer, gamma = .1)\n",
    "# sched = PolynomialLR(optimizer, total_iters=5, )\n",
    "sched = Schedulers.cosine.value(optimizer, T_0= 10)\n",
    "lrs = []\n",
    "steps = 5\n",
    "for _ in range(steps):\n",
    "    sched.step()\n",
    "    lrs.extend(sched.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = range(steps), y = lrs)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = ['resize:224','crop','vertical','horizontal','rotate']\n",
    "l2 = ['crop','vertical','horizontal','rotate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = vit_b_16(weights = ViT_B_16_Weights, image_size = 240)\n",
    "model = vit_b_16(image_size = 240, num_classes =7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "                            Rxrx1(split = 'test', num_categories= 5,\n",
    "                                    data_transform= transforms.Resize(32)), \n",
    "                            batch_size=10, \n",
    "                            shuffle=False, \n",
    "                            num_workers=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = iter(DataLoader(\n",
    "                            Rxrx1(split = 'test', num_categories= 5,\n",
    "                                    data_transform= transforms.Resize(32)), \n",
    "                            batch_size=10, \n",
    "                            shuffle=False, \n",
    "                            num_workers=1\n",
    "                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/jasonwang/github/recursion/src/dataset.py\", line 99, in __getitem__\n    x = torch.cat([self.transform(Image.open(p)) for p in full_paths], dim =0)\nTypeError: expected Tensor as element 0 in argument 0, but got Image\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/jasonwang/github/recursion/src/dataset.py\", line 99, in __getitem__\n    x = torch.cat([self.transform(Image.open(p)) for p in full_paths], dim =0)\nTypeError: expected Tensor as element 0 in argument 0, but got Image\n"
     ]
    }
   ],
   "source": [
    "next(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[int(s.split(':')[-1])for s in l2 if 'resize' in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vit_b_16(image_size = 224,num_classes = 5, dropout = 0.2)# pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "====================\n",
      "Encoder(\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (layers): Sequential(\n",
      "    (encoder_layer_0): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_1): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_2): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_3): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_4): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_5): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_6): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_7): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_8): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_9): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_10): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_11): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "====================\n",
      "Sequential(\n",
      "  (head): Linear(in_features=768, out_features=5, bias=True)\n",
      ")\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for x in model.children():\n",
    "    print(x)\n",
    "    print('=='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (head): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of models failed: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1017, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 947, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/jasonwang/github/recursion/src/models.py\", line 24\n",
      "    class CustomDensenet(nn.Module):\n",
      "IndentationError: expected an indented block after function definition on line 19\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_proj\n",
      "encoder\n",
      "heads\n"
     ]
    }
   ],
   "source": [
    "for c in model.named_children():\n",
    "    # print(len(c))\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config.load_config('/Users/jasonwang/github/recursion/src/example_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "dataset = Rxrx1()\n",
    "cutmix = v2.CutMix(num_classes=config.num_categories)\n",
    "train_collate_fn = lambda batch: cutmix(*default_collate(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Rxrx1(split = 'train', num_categories= config.num_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dataset[0]\n",
    "x, cell_type, labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_list = [dataset[0], dataset[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = default_collate(batch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 384, 384]), torch.Size([2]), torch.Size([2]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex[0].shape, ex[1].shape, ex[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch_list):\n",
    "    # each batch contains: x (img tensor), cell_type (int), label (int)\n",
    "    x_stack = torch.stack([item[0] for item in batch_list], dim = 0)\n",
    "    cell_type_stack = torch.stack([item[1] for item in batch_list], dim = 0)\n",
    "    label_stack = torch.stack([item[2] for item in batch_list], dim = 0)\n",
    "    x_stack, label_stack = cutmix(x_stack, label_stack)\n",
    "    return x_stack, cell_type_stack, label_stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 384, 384])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([x,x,x], dim = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got numpy.int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got numpy.int64"
     ]
    }
   ],
   "source": [
    "default_collate((batch[0], batch[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.meta['sirna_codes'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = iter(DataLoader(Rxrx1(split = 'train', ), batch_size=12, shuffle=True, num_workers=1, collate_fn=train_collate_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex(x,y):\n",
    "    return x**y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pex = partial(ex, y= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pex(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutmix = v2.CutMix(num_classes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = next(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-3.3993e-01, -3.3993e-01, -3.3993e-01,  ..., -5.0203e-01,\n",
       "            -5.0203e-01, -5.0203e-01],\n",
       "           [-1.7783e-01, -1.7783e-01, -1.7783e-01,  ..., -5.0203e-01,\n",
       "            -5.0203e-01, -5.0203e-01],\n",
       "           [-1.5734e-02,  1.4637e-01,  1.4637e-01,  ..., -5.0203e-01,\n",
       "            -5.0203e-01, -5.0203e-01],\n",
       "           ...,\n",
       "           [-5.9867e-01, -5.4380e-01, -5.4380e-01,  ..., -1.5734e-02,\n",
       "            -1.5734e-02, -1.5734e-02],\n",
       "           [-5.9867e-01, -5.4380e-01, -5.4380e-01,  ..., -1.7783e-01,\n",
       "            -1.5734e-02, -1.5734e-02],\n",
       "           [-5.9867e-01, -5.4380e-01, -5.4380e-01,  ..., -1.7783e-01,\n",
       "            -1.7783e-01, -1.5734e-02]],\n",
       " \n",
       "          [[-9.2936e-01, -9.2936e-01, -9.2936e-01,  ..., -8.6784e-02,\n",
       "            -8.6784e-02, -2.5262e-03],\n",
       "           [-9.2936e-01, -9.2936e-01, -9.2936e-01,  ..., -2.5530e-01,\n",
       "            -8.6784e-02, -1.7104e-01],\n",
       "           [-8.4510e-01, -8.4510e-01, -8.4510e-01,  ..., -3.3956e-01,\n",
       "            -3.3956e-01, -5.0807e-01],\n",
       "           ...,\n",
       "           [-8.2419e-01, -9.0069e-01, -9.0069e-01,  ...,  7.5579e-01,\n",
       "             1.1771e+00,  1.3456e+00],\n",
       "           [-9.0069e-01, -9.0069e-01, -8.2419e-01,  ...,  1.1771e+00,\n",
       "             1.4298e+00,  1.5141e+00],\n",
       "           [-9.0069e-01, -9.0069e-01, -9.0069e-01,  ...,  1.5984e+00,\n",
       "             1.7669e+00,  1.9354e+00]],\n",
       " \n",
       "          [[-5.1829e-01, -2.8259e-01,  1.8881e-01,  ..., -1.2254e+00,\n",
       "            -1.2254e+00, -1.2254e+00],\n",
       "           [-2.8259e-01, -2.8259e-01, -4.6886e-02,  ..., -1.2254e+00,\n",
       "            -1.2254e+00, -1.2254e+00],\n",
       "           [ 1.8881e-01, -2.8259e-01, -4.6886e-02,  ..., -1.2254e+00,\n",
       "            -1.2254e+00, -9.8969e-01],\n",
       "           ...,\n",
       "           [ 2.3104e+00,  1.7074e+00,  1.4058e+00,  ..., -2.8259e-01,\n",
       "            -2.8259e-01, -4.6886e-02],\n",
       "           [ 1.8581e+00,  1.5566e+00,  1.4058e+00,  ..., -2.8259e-01,\n",
       "            -5.1829e-01, -2.8259e-01],\n",
       "           [ 1.5566e+00,  1.4058e+00,  1.4058e+00,  ..., -4.6886e-02,\n",
       "            -2.8259e-01, -2.8259e-01]],\n",
       " \n",
       "          [[-9.5982e-01, -9.5982e-01, -9.5982e-01,  ...,  8.3303e-01,\n",
       "             9.9602e-01,  5.0706e-01],\n",
       "           [-9.5982e-01, -9.5982e-01, -9.5982e-01,  ...,  8.3303e-01,\n",
       "             5.0706e-01,  3.4407e-01],\n",
       "           [-9.5982e-01, -9.5982e-01, -9.5982e-01,  ...,  5.0706e-01,\n",
       "             5.0706e-01,  3.4407e-01],\n",
       "           ...,\n",
       "           [ 3.5972e-01,  5.9093e-01,  4.1752e-01,  ..., -1.4489e-01,\n",
       "            -1.4489e-01,  1.8108e-01],\n",
       "           [ 5.3313e-01,  7.0653e-01,  5.3313e-01,  ...,  1.8097e-02,\n",
       "             1.8097e-02,  1.8097e-02],\n",
       "           [ 4.7532e-01,  6.4873e-01,  5.9093e-01,  ...,  3.4407e-01,\n",
       "             1.8108e-01,  1.8108e-01]],\n",
       " \n",
       "          [[-3.5579e-01, -3.5579e-01, -3.5579e-01,  ..., -3.5579e-01,\n",
       "            -3.5579e-01, -3.5579e-01],\n",
       "           [-3.5579e-01, -3.5579e-01, -3.5579e-01,  ..., -3.5579e-01,\n",
       "            -3.5579e-01, -3.5579e-01],\n",
       "           [-3.5579e-01, -3.5579e-01, -3.5579e-01,  ..., -3.5579e-01,\n",
       "            -3.5579e-01, -3.5579e-01],\n",
       "           ...,\n",
       "           [ 5.8725e-01,  3.0902e-01,  3.0783e-02,  ..., -3.0928e-02,\n",
       "            -3.0928e-02,  2.9394e-01],\n",
       "           [ 5.8725e-01,  3.0902e-01, -2.4745e-01,  ...,  2.9394e-01,\n",
       "             6.1880e-01,  9.4367e-01],\n",
       "           [ 5.8725e-01,  3.0902e-01, -2.4745e-01,  ...,  2.9394e-01,\n",
       "             9.4367e-01,  1.2685e+00]],\n",
       " \n",
       "          [[-8.0143e-01, -8.0143e-01, -8.0143e-01,  ...,  1.7856e-01,\n",
       "            -1.4810e-01, -3.1143e-01],\n",
       "           [-8.0143e-01, -8.0143e-01, -8.0143e-01,  ...,  1.5229e-02,\n",
       "            -1.4810e-01, -1.4810e-01],\n",
       "           [-8.0143e-01, -8.0143e-01, -8.0143e-01,  ..., -1.4810e-01,\n",
       "            -1.4810e-01, -1.4810e-01],\n",
       "           ...,\n",
       "           [-5.9030e-01, -5.9030e-01, -8.0681e-01,  ..., -6.3810e-01,\n",
       "            -4.7477e-01, -4.7477e-01],\n",
       "           [-6.9855e-01, -6.9855e-01, -6.9855e-01,  ..., -4.7477e-01,\n",
       "            -4.7477e-01, -4.7477e-01],\n",
       "           [-6.9855e-01, -5.9030e-01, -6.9855e-01,  ..., -3.1143e-01,\n",
       "            -4.7477e-01, -4.7477e-01]]],\n",
       " \n",
       " \n",
       "         [[[-3.9206e-01, -3.9206e-01, -3.9206e-01,  ..., -5.2012e-01,\n",
       "            -5.2012e-01, -5.2012e-01],\n",
       "           [-3.9206e-01, -3.9206e-01, -3.9206e-01,  ..., -5.2012e-01,\n",
       "            -5.2012e-01, -5.2012e-01],\n",
       "           [-3.9206e-01, -3.9206e-01, -3.9206e-01,  ..., -5.2012e-01,\n",
       "            -5.2012e-01, -5.2012e-01],\n",
       "           ...,\n",
       "           [ 1.4637e-01,  1.4637e-01,  1.4637e-01,  ..., -3.9206e-01,\n",
       "            -3.9206e-01, -3.9206e-01],\n",
       "           [ 1.4637e-01, -1.5734e-02,  1.4637e-01,  ..., -3.9206e-01,\n",
       "            -3.9206e-01, -3.9206e-01],\n",
       "           [-1.5734e-02, -1.5734e-02,  1.4637e-01,  ..., -3.9206e-01,\n",
       "            -3.9206e-01, -3.9206e-01]],\n",
       " \n",
       "          [[ 1.0641e+00,  1.0641e+00,  9.1520e-01,  ...,  3.9426e+00,\n",
       "             3.6944e+00,  3.1485e+00],\n",
       "           [ 9.6483e-01,  9.1520e-01,  6.6706e-01,  ...,  3.3470e+00,\n",
       "             2.9996e+00,  2.6026e+00],\n",
       "           [ 6.6706e-01,  6.1743e-01,  5.6780e-01,  ...,  3.0492e+00,\n",
       "             2.7018e+00,  2.1559e+00],\n",
       "           ...,\n",
       "           [-8.4510e-01, -6.7658e-01, -7.6084e-01,  ...,  1.3122e+00,\n",
       "             1.3619e+00,  1.3619e+00],\n",
       "           [-8.4510e-01, -7.6084e-01, -7.6084e-01,  ...,  1.1137e+00,\n",
       "             1.3122e+00,  1.3619e+00],\n",
       "           [-8.4510e-01, -8.4510e-01, -7.6084e-01,  ...,  6.6706e-01,\n",
       "             9.1520e-01,  1.1137e+00]],\n",
       " \n",
       "          [[ 6.1688e-01,  4.1110e-01,  4.1110e-01,  ..., -6.1781e-01,\n",
       "            -4.1203e-01, -4.1203e-01],\n",
       "           [ 4.1110e-01,  2.0532e-01, -4.6603e-04,  ..., -4.1203e-01,\n",
       "            -4.1203e-01, -4.1203e-01],\n",
       "           [ 2.0532e-01,  2.0532e-01, -4.6603e-04,  ..., -2.0625e-01,\n",
       "            -2.0625e-01, -2.0625e-01],\n",
       "           ...,\n",
       "           [-1.2254e+00, -1.2254e+00, -1.2254e+00,  ..., -8.2359e-01,\n",
       "            -8.2359e-01, -8.2359e-01],\n",
       "           [-1.2254e+00, -1.2254e+00, -1.2254e+00,  ..., -8.2359e-01,\n",
       "            -8.2359e-01, -8.2359e-01],\n",
       "           [-1.2254e+00, -1.2254e+00, -1.2254e+00,  ..., -1.0294e+00,\n",
       "            -1.0294e+00, -1.0294e+00]],\n",
       " \n",
       "          [[-1.1821e-01, -1.1821e-01, -2.1080e-01,  ...,  9.0031e-01,\n",
       "             9.0031e-01,  9.0031e-01],\n",
       "           [-1.1821e-01, -2.1080e-01, -3.0340e-01,  ...,  1.0855e+00,\n",
       "             9.0031e-01,  9.0031e-01],\n",
       "           [-2.1080e-01, -2.1080e-01, -3.0340e-01,  ...,  1.0855e+00,\n",
       "             9.9290e-01,  9.9290e-01],\n",
       "           ...,\n",
       "           [ 1.8097e-02,  1.8108e-01, -4.7086e-01,  ..., -2.1080e-01,\n",
       "            -3.0340e-01, -3.0340e-01],\n",
       "           [-4.7086e-01, -3.0788e-01, -4.7086e-01,  ..., -2.1080e-01,\n",
       "            -3.0340e-01, -3.9599e-01],\n",
       "           [-7.9684e-01, -6.3385e-01, -6.3385e-01,  ..., -2.1080e-01,\n",
       "            -3.0340e-01, -3.0340e-01]],\n",
       " \n",
       "          [[-2.3832e-01, -2.3832e-01, -2.3832e-01,  ..., -2.3832e-01,\n",
       "            -2.3832e-01, -2.3832e-01],\n",
       "           [-2.3832e-01, -2.3832e-01, -2.3832e-01,  ..., -6.8348e-01,\n",
       "            -2.3832e-01, -2.3832e-01],\n",
       "           [-2.3832e-01, -2.3832e-01, -2.3832e-01,  ..., -6.8348e-01,\n",
       "            -2.3832e-01, -2.3832e-01],\n",
       "           ...,\n",
       "           [ 6.1880e-01,  2.9394e-01,  2.9394e-01,  ...,  1.0000e+01,\n",
       "             1.1336e+01,  1.0891e+01],\n",
       "           [ 2.9394e-01,  2.9394e-01,  2.9394e-01,  ...,  1.0446e+01,\n",
       "             1.1781e+01,  1.0891e+01],\n",
       "           [ 2.9394e-01,  2.9394e-01,  2.9394e-01,  ...,  1.0446e+01,\n",
       "             1.1336e+01,  1.0446e+01]],\n",
       " \n",
       "          [[-1.5149e+00, -1.5149e+00, -1.5149e+00,  ..., -2.3090e-01,\n",
       "            -7.0400e-02,  9.0103e-02],\n",
       "           [-1.5149e+00, -1.5149e+00, -1.3544e+00,  ..., -2.3090e-01,\n",
       "            -7.0400e-02,  9.0103e-02],\n",
       "           [-1.5149e+00, -1.5149e+00, -1.5149e+00,  ..., -2.3090e-01,\n",
       "            -7.0400e-02,  2.5061e-01],\n",
       "           ...,\n",
       "           [ 5.0522e-01,  6.6855e-01,  8.3189e-01,  ..., -3.9141e-01,\n",
       "            -5.5191e-01, -7.1241e-01],\n",
       "           [ 3.4189e-01,  5.0522e-01,  6.6855e-01,  ..., -5.5191e-01,\n",
       "            -5.5191e-01, -7.1241e-01],\n",
       "           [ 5.0522e-01,  3.4189e-01,  5.0522e-01,  ..., -5.5191e-01,\n",
       "            -5.5191e-01, -8.7292e-01]]],\n",
       " \n",
       " \n",
       "         [[[-7.2093e-02, -3.2713e-01, -3.2713e-01,  ..., -3.2713e-01,\n",
       "            -3.2713e-01, -3.2713e-01],\n",
       "           [-7.2093e-02, -3.2713e-01, -3.2713e-01,  ..., -3.2713e-01,\n",
       "            -3.2713e-01, -3.2713e-01],\n",
       "           [-7.2093e-02, -3.2713e-01, -3.2713e-01,  ..., -3.2713e-01,\n",
       "            -3.2713e-01, -3.2713e-01],\n",
       "           ...,\n",
       "           [-5.2012e-01, -3.9206e-01, -3.9206e-01,  ..., -3.2713e-01,\n",
       "            -3.2713e-01, -3.2713e-01],\n",
       "           [-5.2012e-01, -5.2012e-01, -5.2012e-01,  ..., -3.2713e-01,\n",
       "            -3.2713e-01, -3.2713e-01],\n",
       "           [-5.2012e-01, -5.2012e-01, -5.2012e-01,  ..., -3.2713e-01,\n",
       "            -3.2713e-01, -3.2713e-01]],\n",
       " \n",
       "          [[-7.7327e-01, -7.7327e-01, -7.7327e-01,  ..., -5.9026e-01,\n",
       "            -5.9026e-01, -4.9876e-01],\n",
       "           [-7.7327e-01, -7.7327e-01, -7.7327e-01,  ..., -5.9026e-01,\n",
       "            -5.9026e-01, -5.9026e-01],\n",
       "           [-7.7327e-01, -7.7327e-01, -7.7327e-01,  ..., -5.9026e-01,\n",
       "            -5.9026e-01, -5.9026e-01],\n",
       "           ...,\n",
       "           [-6.2329e-01, -5.7366e-01, -5.7366e-01,  ...,  3.2478e-01,\n",
       "             2.3327e-01,  1.4177e-01],\n",
       "           [-6.7292e-01, -5.7366e-01, -6.2329e-01,  ...,  2.3327e-01,\n",
       "             2.3327e-01,  1.4177e-01],\n",
       "           [-6.7292e-01, -6.2329e-01, -6.2329e-01,  ...,  5.0266e-02,\n",
       "             1.4177e-01,  5.0266e-02]],\n",
       " \n",
       "          [[-1.4120e-01, -3.7942e-01, -1.4120e-01,  ..., -1.5705e+00,\n",
       "            -1.5705e+00, -1.5705e+00],\n",
       "           [-1.4120e-01, -1.4120e-01, -3.7942e-01,  ..., -1.5705e+00,\n",
       "            -1.5705e+00, -1.5705e+00],\n",
       "           [-1.4120e-01, -6.1764e-01, -6.1764e-01,  ..., -1.5705e+00,\n",
       "            -1.5705e+00, -1.5705e+00],\n",
       "           ...,\n",
       "           [ 2.0532e-01, -4.1203e-01, -8.2359e-01,  ..., -1.4120e-01,\n",
       "            -1.4120e-01, -1.4120e-01],\n",
       "           [-4.6603e-04, -6.1781e-01, -6.1781e-01,  ..., -1.4120e-01,\n",
       "            -1.4120e-01, -1.4120e-01],\n",
       "           [-4.6603e-04, -4.1203e-01, -4.1203e-01,  ..., -1.4120e-01,\n",
       "             9.7020e-02, -1.4120e-01]],\n",
       " \n",
       "          [[-5.6945e-01, -4.2237e-01, -4.2237e-01,  ..., -8.6361e-01,\n",
       "            -7.1653e-01, -5.6945e-01],\n",
       "           [-4.2237e-01, -5.6945e-01, -4.2237e-01,  ..., -8.6361e-01,\n",
       "            -7.1653e-01, -5.6945e-01],\n",
       "           [-5.6945e-01, -4.2237e-01, -4.2237e-01,  ..., -8.6361e-01,\n",
       "            -7.1653e-01, -7.1653e-01],\n",
       "           ...,\n",
       "           [-6.7377e-01, -7.6636e-01, -7.6636e-01,  ..., -7.1653e-01,\n",
       "            -5.6945e-01, -5.6945e-01],\n",
       "           [-7.6636e-01, -7.6636e-01, -7.6636e-01,  ..., -7.1653e-01,\n",
       "            -7.1653e-01, -7.1653e-01],\n",
       "           [-7.6636e-01, -7.6636e-01, -7.6636e-01,  ..., -7.1653e-01,\n",
       "            -7.1653e-01, -7.1653e-01]],\n",
       " \n",
       "          [[-2.3064e-01, -7.8445e-01, -7.8445e-01,  ..., -2.3064e-01,\n",
       "            -2.3064e-01, -2.3064e-01],\n",
       "           [-2.3064e-01, -2.3064e-01, -2.3064e-01,  ..., -2.3064e-01,\n",
       "            -2.3064e-01, -2.3064e-01],\n",
       "           [-2.3064e-01, -2.3064e-01, -2.3064e-01,  ..., -2.3064e-01,\n",
       "            -2.3064e-01, -2.3064e-01],\n",
       "           ...,\n",
       "           [-6.8348e-01, -6.8348e-01, -6.8348e-01,  ..., -7.8445e-01,\n",
       "            -7.8445e-01, -7.8445e-01],\n",
       "           [-6.8348e-01, -6.8348e-01, -6.8348e-01,  ..., -7.8445e-01,\n",
       "            -2.3064e-01, -7.8445e-01],\n",
       "           [-6.8348e-01, -6.8348e-01, -6.8348e-01,  ..., -7.8445e-01,\n",
       "            -2.3064e-01, -7.8445e-01]],\n",
       " \n",
       "          [[ 1.3972e+00,  1.7388e+00,  1.3972e+00,  ...,  7.1404e-01,\n",
       "             3.7246e-01,  3.7246e-01],\n",
       "           [ 2.0804e+00,  2.0804e+00,  1.7388e+00,  ...,  3.7246e-01,\n",
       "             7.1404e-01,  3.7246e-01],\n",
       "           [ 2.4220e+00,  2.0804e+00,  2.0804e+00,  ...,  3.7246e-01,\n",
       "             7.1404e-01,  3.7246e-01],\n",
       "           ...,\n",
       "           [-2.3090e-01, -5.5191e-01, -5.5191e-01,  ...,  1.3972e+00,\n",
       "             1.3972e+00,  1.0556e+00],\n",
       "           [-3.9141e-01, -5.5191e-01, -7.1241e-01,  ...,  7.1404e-01,\n",
       "             1.0556e+00,  1.0556e+00],\n",
       "           [-7.1241e-01, -5.5191e-01, -7.1241e-01,  ...,  7.1404e-01,\n",
       "             1.0556e+00,  1.0556e+00]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-4.2016e-01, -4.2016e-01, -4.2016e-01,  ..., -4.2016e-01,\n",
       "            -4.2016e-01, -4.2016e-01],\n",
       "           [-4.2016e-01, -4.2016e-01, -4.2016e-01,  ..., -4.2016e-01,\n",
       "            -4.2016e-01, -4.2016e-01],\n",
       "           [-4.2016e-01, -4.2016e-01, -4.2016e-01,  ..., -4.2016e-01,\n",
       "            -4.2016e-01, -4.2016e-01],\n",
       "           ...,\n",
       "           [ 2.9016e+00,  2.4743e+00,  1.9484e+00,  ..., -4.2016e-01,\n",
       "            -4.2016e-01, -4.2016e-01],\n",
       "           [ 3.3618e+00,  2.6058e+00,  1.6525e+00,  ..., -4.2016e-01,\n",
       "            -4.2016e-01, -4.2016e-01],\n",
       "           [ 3.6577e+00,  2.6715e+00,  1.8826e+00,  ..., -4.2016e-01,\n",
       "            -4.2016e-01, -4.2016e-01]],\n",
       " \n",
       "          [[ 3.9739e-02, -3.0302e-02, -3.0302e-02,  ...,  8.8023e-01,\n",
       "             1.4406e+00,  1.7207e+00],\n",
       "           [ 1.7982e-01,  1.0978e-01,  1.0978e-01,  ...,  1.0904e+00,\n",
       "             1.5806e+00,  1.6507e+00],\n",
       "           [ 3.8994e-01,  3.8994e-01,  3.1990e-01,  ...,  1.2304e+00,\n",
       "             1.4406e+00,  1.2304e+00],\n",
       "           ...,\n",
       "           [-6.6724e-01, -7.4350e-01, -7.4350e-01,  ..., -1.0034e-01,\n",
       "             1.0978e-01,  1.0978e-01],\n",
       "           [-7.4350e-01, -7.4350e-01, -7.4350e-01,  ..., -1.0034e-01,\n",
       "             1.7982e-01, -3.0302e-02],\n",
       "           [-7.4350e-01, -7.4350e-01, -7.4350e-01,  ..., -2.4042e-01,\n",
       "             3.9739e-02, -1.0034e-01]],\n",
       " \n",
       "          [[ 2.1835e-02,  2.1835e-02,  2.1835e-02,  ...,  2.1835e-02,\n",
       "             2.1835e-02,  2.1835e-02],\n",
       "           [-3.1120e-01, -3.1120e-01, -3.1120e-01,  ...,  2.1835e-02,\n",
       "             6.8790e-01,  6.8790e-01],\n",
       "           [-6.4423e-01, -6.4423e-01, -3.1120e-01,  ...,  3.5487e-01,\n",
       "             3.5487e-01,  6.8790e-01],\n",
       "           ...,\n",
       "           [-9.7657e-01, -9.7657e-01, -9.7657e-01,  ..., -3.1120e-01,\n",
       "            -3.1120e-01, -3.1120e-01],\n",
       "           [-9.7657e-01, -9.7657e-01, -9.7657e-01,  ..., -3.1120e-01,\n",
       "            -3.1120e-01, -3.1120e-01],\n",
       "           [-8.6228e-01, -7.4799e-01, -6.3370e-01,  ..., -3.1120e-01,\n",
       "            -6.4423e-01, -6.4423e-01]],\n",
       " \n",
       "          [[-1.5933e-01, -2.9590e-01, -2.9590e-01,  ..., -4.3246e-01,\n",
       "            -4.3246e-01, -1.5933e-01],\n",
       "           [-2.9590e-01, -2.9590e-01, -2.9590e-01,  ..., -4.3246e-01,\n",
       "            -2.9590e-01, -2.9590e-01],\n",
       "           [-2.9590e-01, -2.9590e-01, -2.9590e-01,  ..., -2.9590e-01,\n",
       "            -2.2771e-02,  1.1379e-01],\n",
       "           ...,\n",
       "           [-8.3652e-01, -6.9078e-01, -2.0497e-01,  ..., -2.9590e-01,\n",
       "            -2.9590e-01, -2.9590e-01],\n",
       "           [-7.8794e-01, -7.3936e-01, -3.9929e-01,  ..., -4.3246e-01,\n",
       "            -4.3246e-01, -4.3246e-01],\n",
       "           [-7.3936e-01, -7.8794e-01, -7.3936e-01,  ..., -4.3246e-01,\n",
       "            -4.3246e-01, -4.3246e-01]],\n",
       " \n",
       "          [[ 1.0941e+00,  1.0941e+00,  1.0941e+00,  ...,  2.8725e-01,\n",
       "            -1.1617e-01, -5.1959e-01],\n",
       "           [ 6.9067e-01,  6.9067e-01,  6.9067e-01,  ...,  2.8725e-01,\n",
       "            -5.1959e-01, -5.1959e-01],\n",
       "           [ 6.9067e-01,  1.0941e+00,  2.8725e-01,  ..., -1.1617e-01,\n",
       "            -1.1617e-01, -1.1617e-01],\n",
       "           ...,\n",
       "           [ 4.2187e-01,  4.2187e-01,  9.8074e-01,  ...,  2.8725e-01,\n",
       "             2.8725e-01,  6.9067e-01],\n",
       "           [ 7.0130e-01,  9.8074e-01,  1.2602e+00,  ...,  2.8725e-01,\n",
       "             2.8725e-01,  2.8725e-01],\n",
       "           [ 9.8074e-01,  1.1205e+00,  1.3999e+00,  ...,  2.8725e-01,\n",
       "             2.8725e-01,  2.8725e-01]],\n",
       " \n",
       "          [[ 4.2228e-01, -6.9066e-02,  4.2228e-01,  ...,  3.3704e+00,\n",
       "             3.3704e+00,  4.3531e+00],\n",
       "           [ 4.2228e-01, -6.9066e-02, -6.9066e-02,  ...,  2.8790e+00,\n",
       "             2.8790e+00,  3.8617e+00],\n",
       "           [-6.9066e-02, -6.9066e-02, -6.9066e-02,  ...,  2.8790e+00,\n",
       "             2.8790e+00,  3.3704e+00],\n",
       "           ...,\n",
       "           [-1.3605e+00, -1.2053e+00, -1.0502e+00,  ..., -6.9066e-02,\n",
       "             4.2228e-01, -6.9066e-02],\n",
       "           [-1.4380e+00, -1.2829e+00, -1.2053e+00,  ..., -5.6042e-01,\n",
       "            -6.9066e-02, -6.9066e-02],\n",
       "           [-1.5156e+00, -1.3605e+00, -1.3605e+00,  ..., -5.6042e-01,\n",
       "            -6.9066e-02, -5.6042e-01]]],\n",
       " \n",
       " \n",
       "         [[[-3.9943e-01, -3.9943e-01, -3.9943e-01,  ...,  2.8898e+00,\n",
       "             2.4786e+00,  2.0675e+00],\n",
       "           [-3.9943e-01, -3.9943e-01, -3.9943e-01,  ...,  2.8898e+00,\n",
       "             2.4786e+00,  2.4786e+00],\n",
       "           [-3.9943e-01, -3.9943e-01, -3.9943e-01,  ...,  2.8898e+00,\n",
       "             2.4786e+00,  2.4786e+00],\n",
       "           ...,\n",
       "           [-4.2016e-01, -5.6070e-01, -5.6070e-01,  ...,  4.2287e-01,\n",
       "             4.2287e-01,  4.2287e-01],\n",
       "           [-5.6070e-01, -5.6070e-01, -5.6070e-01,  ...,  4.2287e-01,\n",
       "             4.2287e-01,  4.2287e-01],\n",
       "           [-5.6070e-01, -5.6070e-01, -5.6070e-01,  ...,  4.2287e-01,\n",
       "             4.2287e-01,  4.2287e-01]],\n",
       " \n",
       "          [[-7.1314e-01, -7.1314e-01, -7.1314e-01,  ..., -5.1174e-01,\n",
       "            -5.1174e-01, -5.1174e-01],\n",
       "           [-7.1314e-01, -6.1244e-01, -6.1244e-01,  ..., -4.1104e-01,\n",
       "            -4.1104e-01, -5.1174e-01],\n",
       "           [-6.1244e-01, -6.1244e-01, -6.1244e-01,  ..., -3.1034e-01,\n",
       "            -3.1034e-01, -4.1104e-01],\n",
       "           ...,\n",
       "           [-6.6067e-01, -7.3071e-01, -8.0075e-01,  ...,  1.6030e+00,\n",
       "             1.9051e+00,  2.1065e+00],\n",
       "           [-6.6067e-01, -8.0075e-01, -8.0075e-01,  ...,  2.0058e+00,\n",
       "             2.2072e+00,  2.3079e+00],\n",
       "           [-8.0075e-01, -8.0075e-01, -8.7079e-01,  ...,  2.6100e+00,\n",
       "             3.0128e+00,  2.9121e+00]],\n",
       " \n",
       "          [[ 9.4318e-01,  4.2899e-01,  4.2899e-01,  ..., -8.5200e-02,\n",
       "            -8.5200e-02, -8.5200e-02],\n",
       "           [ 9.4318e-01,  9.4318e-01,  9.4318e-01,  ..., -8.5200e-02,\n",
       "             4.2899e-01,  4.2899e-01],\n",
       "           [ 1.4574e+00,  1.4574e+00,  1.4574e+00,  ...,  4.2899e-01,\n",
       "             9.4318e-01,  9.4318e-01],\n",
       "           ...,\n",
       "           [-3.1120e-01, -3.1120e-01, -3.1120e-01,  ...,  4.2899e-01,\n",
       "             4.2899e-01,  4.2899e-01],\n",
       "           [-3.1120e-01, -6.4423e-01, -6.4423e-01,  ..., -8.5200e-02,\n",
       "            -8.5200e-02, -8.5200e-02],\n",
       "           [-3.1120e-01, -3.1120e-01, -6.4423e-01,  ..., -8.5200e-02,\n",
       "            -8.5200e-02, -8.5200e-02]],\n",
       " \n",
       "          [[-6.0331e-01, -8.5339e-01, -8.5339e-01,  ...,  3.9704e-01,\n",
       "             3.9704e-01,  1.4695e-01],\n",
       "           [-6.0331e-01, -8.5339e-01, -8.5339e-01,  ...,  1.4695e-01,\n",
       "            -1.0313e-01,  1.4695e-01],\n",
       "           [-6.0331e-01, -6.0331e-01, -8.5339e-01,  ..., -1.0313e-01,\n",
       "            -1.0313e-01, -1.0313e-01],\n",
       "           ...,\n",
       "           [ 1.1379e-01,  1.1379e-01,  5.2348e-01,  ...,  1.6475e+00,\n",
       "             2.1476e+00,  2.6478e+00],\n",
       "           [ 1.1379e-01,  2.5036e-01,  6.6005e-01,  ...,  2.3977e+00,\n",
       "             2.8979e+00,  3.1480e+00],\n",
       "           [ 5.2348e-01,  5.2348e-01,  6.6005e-01,  ...,  2.8979e+00,\n",
       "             3.1480e+00,  2.8979e+00]],\n",
       " \n",
       "          [[ 5.4091e-01,  2.3349e-01,  2.3349e-01,  ...,  5.4091e-01,\n",
       "             5.4091e-01,  8.4832e-01],\n",
       "           [ 8.4832e-01,  5.4091e-01,  2.3349e-01,  ...,  8.4832e-01,\n",
       "             5.4091e-01,  8.4832e-01],\n",
       "           [ 1.1557e+00,  8.4832e-01,  2.3349e-01,  ...,  5.4091e-01,\n",
       "             5.4091e-01,  8.4832e-01],\n",
       "           ...,\n",
       "           [-9.2302e-01, -9.2302e-01, -9.2302e-01,  ..., -3.8134e-01,\n",
       "            -3.8134e-01, -3.8134e-01],\n",
       "           [-9.2302e-01, -9.2302e-01, -9.2302e-01,  ..., -3.8134e-01,\n",
       "            -3.8134e-01, -3.8134e-01],\n",
       "           [-9.2302e-01, -9.2302e-01, -9.2302e-01,  ..., -3.8134e-01,\n",
       "            -3.8134e-01, -3.8134e-01]],\n",
       " \n",
       "          [[-1.5739e+00, -1.5739e+00, -7.1325e-01,  ...,  1.4741e-01,\n",
       "             1.4741e-01,  1.4741e-01],\n",
       "           [-1.5739e+00, -7.1325e-01, -1.5739e+00,  ...,  1.4741e-01,\n",
       "             1.4741e-01,  1.4741e-01],\n",
       "           [-7.1325e-01, -1.5739e+00, -1.5739e+00,  ...,  1.4741e-01,\n",
       "             1.4741e-01,  1.4741e-01],\n",
       "           ...,\n",
       "           [ 9.1363e-01,  9.1363e-01,  4.2228e-01,  ...,  1.0081e+00,\n",
       "             1.8687e+00,  1.8687e+00],\n",
       "           [ 4.2228e-01,  4.2228e-01, -6.9066e-02,  ...,  2.7294e+00,\n",
       "             2.7294e+00,  2.7294e+00],\n",
       "           [ 4.2228e-01, -6.9066e-02, -6.9066e-02,  ...,  1.8687e+00,\n",
       "             1.0081e+00,  1.0081e+00]]],\n",
       " \n",
       " \n",
       "         [[[-2.1462e-01, -2.1462e-01, -1.5976e-01,  ..., -3.7921e-01,\n",
       "            -3.2435e-01, -2.1462e-01],\n",
       "           [-2.1462e-01, -2.1462e-01, -1.5976e-01,  ..., -3.7921e-01,\n",
       "            -2.6949e-01, -2.1462e-01],\n",
       "           [-2.1462e-01, -2.1462e-01, -2.1462e-01,  ..., -3.7921e-01,\n",
       "            -2.6949e-01, -2.1462e-01],\n",
       "           ...,\n",
       "           [ 1.1722e-02,  1.1722e-02,  1.1722e-02,  ..., -5.4380e-01,\n",
       "            -5.4380e-01, -5.4380e-01],\n",
       "           [ 1.1722e-02,  1.1722e-02,  1.1722e-02,  ..., -5.4380e-01,\n",
       "            -5.4380e-01, -5.4380e-01],\n",
       "           [ 1.1722e-02,  1.1722e-02,  1.1722e-02,  ..., -5.4380e-01,\n",
       "            -5.4380e-01, -5.4380e-01]],\n",
       " \n",
       "          [[-7.4770e-01, -7.4770e-01, -7.4770e-01,  ..., -5.9470e-01,\n",
       "            -6.7120e-01, -7.4770e-01],\n",
       "           [-7.4770e-01, -7.4770e-01, -7.4770e-01,  ..., -5.1820e-01,\n",
       "            -5.9470e-01, -7.4770e-01],\n",
       "           [-7.4770e-01, -7.4770e-01, -6.7120e-01,  ..., -4.4170e-01,\n",
       "            -6.7120e-01, -7.4770e-01],\n",
       "           ...,\n",
       "           [-5.1174e-01, -5.1174e-01, -5.1174e-01,  ..., -9.7719e-01,\n",
       "            -9.7719e-01, -9.7719e-01],\n",
       "           [-5.1174e-01, -6.1244e-01, -5.1174e-01,  ..., -9.7719e-01,\n",
       "            -9.7719e-01, -9.7719e-01],\n",
       "           [-5.1174e-01, -5.1174e-01, -5.1174e-01,  ..., -9.7719e-01,\n",
       "            -9.7719e-01, -9.7719e-01]],\n",
       " \n",
       "          [[ 3.5043e-01,  6.5197e-01,  9.5351e-01,  ..., -2.5265e-01,\n",
       "            -4.0342e-01, -1.0188e-01],\n",
       "           [ 1.2550e+00,  2.0089e+00,  1.4058e+00,  ...,  4.8889e-02,\n",
       "            -2.5265e-01, -2.5265e-01],\n",
       "           [ 1.1043e+00,  1.7074e+00,  1.1043e+00,  ...,  4.8889e-02,\n",
       "            -1.0188e-01, -1.0188e-01],\n",
       "           ...,\n",
       "           [-1.1136e+00, -1.1136e+00, -5.9939e-01,  ..., -1.0188e-01,\n",
       "            -5.5419e-01, -7.0496e-01],\n",
       "           [-1.1136e+00, -5.9939e-01, -5.9939e-01,  ..., -2.5265e-01,\n",
       "            -4.0342e-01, -7.0496e-01],\n",
       "           [-5.9939e-01, -5.9939e-01, -5.9939e-01,  ..., -4.0342e-01,\n",
       "            -7.0496e-01, -1.0065e+00]],\n",
       " \n",
       "          [[ 9.9554e-01,  8.7994e-01,  3.5972e-01,  ...,  1.2912e-02,\n",
       "             4.1752e-01,  5.9093e-01],\n",
       "           [ 5.3313e-01,  1.8632e-01, -4.4890e-02,  ...,  4.1752e-01,\n",
       "             4.1752e-01,  5.3313e-01],\n",
       "           [-3.9170e-01, -5.0730e-01, -5.6510e-01,  ...,  3.5972e-01,\n",
       "             4.1752e-01,  5.9093e-01],\n",
       "           ...,\n",
       "           [ 6.4713e-01,  3.9704e-01,  3.9704e-01,  ...,  1.1689e+00,\n",
       "             9.9554e-01,  7.6433e-01],\n",
       "           [ 6.4713e-01,  3.9704e-01,  1.4695e-01,  ...,  9.3774e-01,\n",
       "             7.0653e-01,  5.9093e-01],\n",
       "           [ 6.4713e-01,  1.4695e-01, -1.0313e-01,  ...,  7.0653e-01,\n",
       "             5.3313e-01,  6.4873e-01]],\n",
       " \n",
       "          [[ 3.0783e-02,  3.0783e-02,  3.0902e-01,  ..., -1.0822e+00,\n",
       "            -8.0392e-01, -8.0392e-01],\n",
       "           [ 3.0902e-01,  3.0902e-01,  5.8725e-01,  ..., -1.0822e+00,\n",
       "            -1.0822e+00, -8.0392e-01],\n",
       "           [ 3.0783e-02,  3.0783e-02,  5.8725e-01,  ..., -1.0822e+00,\n",
       "            -1.0822e+00, -1.0822e+00],\n",
       "           ...,\n",
       "           [-6.8875e-01, -6.8875e-01, -6.8875e-01,  ..., -1.0822e+00,\n",
       "            -8.0392e-01, -8.0392e-01],\n",
       "           [-6.8875e-01, -6.8875e-01, -6.8875e-01,  ..., -1.0822e+00,\n",
       "            -8.0392e-01, -1.0822e+00],\n",
       "           [-6.8875e-01, -6.8875e-01, -6.8875e-01,  ..., -1.0822e+00,\n",
       "            -1.0822e+00, -8.0392e-01]],\n",
       " \n",
       "          [[-3.7378e-01, -3.7378e-01, -4.8204e-01,  ...,  3.8402e-01,\n",
       "             6.0054e-01,  8.1705e-01],\n",
       "           [-4.8204e-01, -4.8204e-01, -4.8204e-01,  ...,  6.0054e-01,\n",
       "             6.0054e-01,  8.1705e-01],\n",
       "           [-6.9855e-01, -6.9855e-01, -5.9030e-01,  ...,  8.1705e-01,\n",
       "             9.2531e-01,  9.2531e-01],\n",
       "           ...,\n",
       "           [-7.1325e-01, -7.1325e-01,  1.4741e-01,  ...,  1.0336e+00,\n",
       "             1.0336e+00,  1.2501e+00],\n",
       "           [-7.1325e-01, -7.1325e-01,  1.4741e-01,  ...,  1.0336e+00,\n",
       "             1.0336e+00,  1.0336e+00],\n",
       "           [-7.1325e-01,  1.4741e-01,  1.4741e-01,  ...,  1.1418e+00,\n",
       "             1.1418e+00,  1.1418e+00]]]]),\n",
       " tensor([[0.4473, 0.5527, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5527, 0.4473, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4473, 0.5527, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.4473, 0.5527, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.5527, 0.4473, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5527, 0.4473, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4473, 0.5527, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5527, 0.4473, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutmix(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m q,w,e \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m),\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "q,w,e = ((1,2),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mq\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'q' is not defined"
     ]
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.int64, torch.int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype, b.dtype, c.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "INFO:train:Epoch 0\n",
      "0it [00:04, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/recursion/src/train.py:90\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     87\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     89\u001b[0m x, labels \u001b[38;5;241m=\u001b[39m cutmix(x, labels)\n\u001b[0;32m---> 90\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m pred_metric_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmetric_classify(embedding)\n\u001b[1;32m     92\u001b[0m pred_cftn \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mclassification(embedding)\n",
      "File \u001b[0;32m~/github/recursion/src/models.py:45\u001b[0m, in \u001b[0;36mCustomDensenet.embed\u001b[0;34m(self, x, s)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, s):\n\u001b[0;32m---> 45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# uses embedding backbone, (bs, 1024, 16,16)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# maybe dontuse adaptive pooling here?\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(x, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# (bs, 1024, 1, 1)        \u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of config failed: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/diffusion/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1017, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 947, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/jasonwang/github/recursion/src/config.py\", line 26\n",
      "    data_augmentation: List[Literal['vertical', 'horizontal', 'crop:284', 'rotate', 'cutmix']],\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "SyntaxError: non-default argument follows default argument\n",
      "]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_augmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvertical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/recursion/src/config.py:19\u001b[0m, in \u001b[0;36mConfig.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_pipeline \u001b[38;5;241m=\u001b[39m make_transform_pipeline(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_augmentation)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cutmix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcutmix\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_augmentation\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "Config(data_augmentation=['vertical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['data_augmentation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basic_augmentations = set(['vertical', 'horizontal', 'crop:284', 'rotate', 'cutmix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arst'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['rotate','arst']) - basic_augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomDensenet(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model('densenet121')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(torchvision.models, 'densenet121')(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'ARST'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
